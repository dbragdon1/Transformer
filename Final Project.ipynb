{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder():\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "e = Embedder(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello': 0, 'I': 1, 'am': 2, 'Dylan': 3}\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"Hello\", \"I\", \"am\", \"Dylan\"]\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "print(word_to_ix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.03762\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\n",
    "https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8\n",
    "\n",
    "https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the **positional encoding** step of the Transformer. It is needed to let the model understand the relative positions of each word in the embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "        #initialize positional encoding matrix to contain all zeroes\n",
    "        #max_seq_len = longest sentence we expect to observe in embedding matrix\n",
    "        \n",
    "        pos_encoding_mat = torch.zeros(max_seq_len, d_model)\n",
    "    \n",
    "        #iterate over one sentence position at a time\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            \n",
    "            #iterate over each sentence at current pos\n",
    "            #Step size is 2 because you have to append cos sinusoid next to sin sinusoid\n",
    "            for i in range(0, d_model, 2):\n",
    "                #Create sinusoid for each entry in embedding matrix\n",
    "                #pos = embedding matrix row (embedded word location)\n",
    "                #i = embedding matrix column (embedding vector location)\n",
    "                pos_encoding_mat[pos, i] = math.sin(pos / (np.power(10000, (2 * i) / d_model)))\n",
    "                pos_encoding_mat[pos, i + 1] = math.cos(pos / (np.power(10000, (2 * (i+1)) / d_model)))\n",
    "                \n",
    "                pos_encoding_mat[pos, i] = 1 \n",
    "        \n",
    "        pos_encoding_mat = pos_encoding_mat.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pos_encoding_mat', pos_encoding_mat)\n",
    "        print(pos_encoding_mat)\n",
    "        \n",
    "    #function to add positional encoding matrix to embedding matrix\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pos_encoding_mat[:,:x.size(1)], requires_grad = False)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.9875,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.9502,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.8891,  1.0000,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.8057,  1.0000,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.7021,  1.0000,  0.9998,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.5809,  1.0000,  0.9997,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.4452,  1.0000,  0.9996,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.2983,  1.0000,  0.9995,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000,  0.1439,  1.0000,  0.9994,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.0141,  1.0000,  0.9992,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000],\n",
      "         [ 1.0000, -0.1717,  1.0000,  0.9990,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-063b29c70039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-a8b539110a2e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#function to add positional encoding matrix to embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "#10 columns\n",
    "#longest sequence will be length 500\n",
    "en = PositionalEncoder(10, 12)\n",
    "x = torch.zeros(10,12)\n",
    "en.forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions (# columns) of the Query, Value, and Key matrices are hyperparameters. Also, if there are $n$ words in a sequence (input sentence), then the $Q$ ,$V$, and $K$ matrices will have $n$ rows. After $matmul(Q, K)$, the dimensionality of the resulting matrix will be $n \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5220, 0.5080, 0.4684]]) tensor([[0.5964],\n",
      "        [0.1537],\n",
      "        [0.5928]])\n",
      "tensor([[0.4996, 0.6760, 1.0904],\n",
      "        [0.2569, 0.6730, 0.7768],\n",
      "        [0.4977, 0.6760, 1.0879]], grad_fn=<ThAddmmBackward>) tensor([[ 0.6447, -0.9489, -0.4299],\n",
      "        [ 0.2664, -0.6795, -0.2627],\n",
      "        [ 0.6417, -0.9467, -0.4286]], grad_fn=<ThAddmmBackward>) tensor([[ 0.6447, -0.9489, -0.4299],\n",
      "        [ 0.2664, -0.6795, -0.2627],\n",
      "        [ 0.6417, -0.9467, -0.4286]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, input_dim, key_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.input_dim = input_dim\n",
    "        self.key_dim = key_dim\n",
    "        \n",
    "        #Query, value, and key matrices\n",
    "        #first arg: hidden size (# of words in input)\n",
    "        #second arg: feature size (hyperparameter)\n",
    "        self.W_Q = nn.Linear(input_dim, key_dim)\n",
    "        self.W_V = nn.Linear(input_dim, key_dim)\n",
    "        self.W_K = nn.Linear(input_dim, key_dim)\n",
    "    \n",
    "    def generate_heads(self, x):\n",
    "        Q = self.W_Q(x)\n",
    "        V = self.W_V(x)\n",
    "        K = self.W_V(x)\n",
    "        print(Q,V,K)\n",
    "        #print(\"Generating \" + str(self.heads) + \" heads\") \n",
    "        #for tensor in [self.W_Q, self.W_V, self.W_K]:\n",
    "        #    torch.split(tensor, self.heads, dim=1)\n",
    "        \n",
    "    def ScaledDotAttention(self, embeddings, Q, V, K, d_K):\n",
    "        K_T = torch.transpose(torch.matmul(embeddings, W_K), 0, -1)\n",
    "        soft = F.softmax(K_T / np.sqrt(d_K), -1)\n",
    "        return torch.matmul(soft, V)   \n",
    "    \n",
    "W = torch.rand(1,3)\n",
    "Q = torch.rand(1,3)\n",
    "V = torch.rand(1,3)\n",
    "\n",
    "word = torch.rand(3,1)\n",
    "print(W, word)\n",
    "\n",
    "m = MultiHeadAttention(6, 1, 3)\n",
    "\n",
    "m.generate_heads(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4434,  0.1025,  0.4296, -0.7735, -0.8767],\n",
      "        [ 0.0263, -0.1078, -0.1602, -0.4200, -0.7669]],\n",
      "       grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "V = nn.Linear(2,5)\n",
    "x = torch.rand(2,2)\n",
    "print(V(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
