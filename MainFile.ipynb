{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    #Constructor\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to embed each word in the corpus, based on semantic similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def__init(self, vocab_size, dimensionality):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard RNN architectures typically fall short at capturing the meaning of really long sentences. In **attention mechanisms**, each output word depends on a weighted combination of each input word, rather than just the final hidden state from a standard RNN. The weights tell the decoder which state to pay most attention to when generating each word in the output sentence. \n",
    "\n",
    "**Multi-headed attention** is the process of splitting up the final hidden state into equal sized chunks. Attention is applied to each chunk in parallel, then the chunks are concatenated. It is typically used to give the attention layer multiple \"representation subspaces.\" This is actually the main component of the transformer model. \n",
    "\n",
    "Our paper uses a specific form of attention, called **scaled dot-product attention**. The equation is: \n",
    "\n",
    "$$Attention(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = softmax(\\frac{\\textbf{QK}^{\\top}}{\\sqrt{n}})\\textbf{V}$$\n",
    "\n",
    "where $n = dim(K)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.Module is the base class referenced for all neural network modules in pytorch\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = heads\n",
    "        self.d_K = np.power(d_model, 1/heads)\n",
    "        \n",
    "        #Q, V, and K matrices used in attention functions\n",
    "        self.Q_linear = nn.Linear(d_model, d_model)\n",
    "        self.V_linear = nn.Linear(d_model, d_model)\n",
    "        self.K_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        #dropout prevents overfitting  during the attention stage\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #attention_scores = attention(q, k, d_k, mask=None, dropout=None)\n",
    "    \n",
    "    #d_k is the square root of the dimension of the key vectors\n",
    "    #For some reason the paper uses a dimension of 64 so we're going with that \n",
    "    \n",
    "    #each word in the sentence vector is going to have its own Q, K, and V value. \n",
    "    def Attention(self, Q, K, V, d_K, dropout=None):\n",
    "        scores = F.softmax(torch.matmul(Q, K.transpose(0,1)) / np.sqrt(d_K), dim = 0)\n",
    "        scores = scores * V\n",
    "        print(\"Attention Scores: \")\n",
    "        return scores\n",
    "        #score vector should be same dimensions as Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First parameter: Number of heads\n",
    "#Second parameter: Dimension of model\n",
    "net = MultiHeadedAttention(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python lign167",
   "language": "python",
   "name": "lign167"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
