{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    #Constructor\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard RNN architectures typically fall short at capturing the meaning of really long sentences. In **attention mechanisms**, each output word depends on a weighted combination of each input word, rather than just the final hidden state from a standard RNN. The weights tell the decoder which state to pay most attention to when generating each word in the output sentence. \n",
    "\n",
    "**Multi-headed attention** is the process of splitting up the final hidden state into equal sized chunks. Attention is applied to each chunk in parallel, then the chunks are concatenated. It is typically used to give the attention layer multiple \"representation subspaces.\" This is actually the main component of the transformer model. \n",
    "\n",
    "Our paper uses a specific form of attention, called **scaled dot-product attention**. The equation is: \n",
    "\n",
    "$$Attention(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = softmax(\\frac{\\textbf{QK}^{\\top}}{\\sqrt{n}})\\textbf{V}$$\n",
    "\n",
    "where $n = dim(K)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = heads\n",
    "        self.d_K = np.power(d_model, 1/heads)\n",
    "        \n",
    "        #Q, V, and K matrices used in attention functions\n",
    "        self.Q_linear = nn.Linear(d_model, d_model)\n",
    "        self.V_linear = nn.Linear(d_model, d_model)\n",
    "        self.K_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        #dropout prevents overfitting  during the attention stage\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #attention_scores = attention(q, k, d_k, mask=None, dropout=None)\n",
    "        \n",
    "        def Attention(Q, K, V, d_K, dropout=None):\n",
    "            scores = F.softmax(torch.matmul(Q, K.transpose(0,1)) / np.sqrt(d_K), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python lign167",
   "language": "python",
   "name": "lign167"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
